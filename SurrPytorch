import numpy as np
import torch
from matplotlib.pyplot import figure,show
import pyDOE
from pyDOE import *
from scipy.stats.distributions import norm
from profilestats import profile
import sklearn

def meshtruss(p1,p2,nx,ny):
  nodes = []
  bars = []
  xx = np.linspace(p1[0],p2[0],nx+1)
  yy = np.linspace(p1[1],p2[1],ny+1)
  for y in yy:
    for x in xx:
      nodes.append([x,y])
  for j in range(ny):
      for i in range(nx):
        n1 = i + j*(nx+1)
        n2 = n1 + 1
        n3 = n1 + nx + 1
        n4 = n3 + 1
        bars.extend([[n1,n2],[n1,n3],[n1,n4],[n2,n3]])
      bars.append([n2,n4])
  index = ny*(nx+1) + 1
  for j in range(nx):
    bars.append([index+j-1,index+j])
  return np.array(nodes), np.array(bars)


def Surrogate(coord,connec,E,F,freenode,samples=100,testratio=0.2):
    # Defining Member Properties
    n = connec.shape[0]  # Number of members
    m = coord.shape[0]   # Number of nodes
    vectors = coord[connec[:,1],:] - coord[connec[:,0],:]  # Nonnormalised direction cosines
    l = np.sqrt((vectors**2).sum(axis=1)) # Length
    e = vectors.T/l # Normalised Direction Cosines
    B = (e[np.newaxis] * e[:,np.newaxis]).T # Transformation Matrix

    # Defining Structure Stiffness Matrix
    def K(x):
        D = E * x/l
        kx = e * D
        K = np.zeros((2*m, 2*m))
        for i in range(n): # Local Stiffness Matrices
          aux = 2*connec[i,:]
          index = np.r_[aux[0]:aux[0]+2, aux[1]:aux[1]+2]
          k0 = np.concatenate((np.concatenate((B[i],-B[i]),axis=1), \
          np.concatenate((-B[i],B[i]),axis=1)), axis=0)
          K[np.ix_(index,index)] = K[np.ix_(index,index)] + D[i] * k0
        block = freenode.flatten().nonzero()[0]
        matrix = K[np.ix_(block,block)]
        return matrix


    block = freenode.flatten().nonzero()[0]
    rhs = F.flatten()[block]

    nsurr=n # Design Variables
    m_train=int((1-testratio)*samples)
    x=np.random.random(n)
    dof= rhs.shape[0] # Degree of freedom

    #Generating training inputs
    X = lhs(nsurr, samples=m_train) # Latin Hypercube sampling with uniform Distribution
    #X = norm(loc=0, scale=1).ppf(X)  # Normal Distribution

    #means = [1, 2, 3, 4]
    #stdvs = [0.1, 0.5, 1, 0.25]
    #for i in xrange(4):
      #X[:, i] = norm(loc=means[i], scale=stdvs[i]).ppf(X[:, i])#

    #Generating training outputs

    U=np.ones((len(X),dof))
    for i in range(len(X)):
      U[i]=np.linalg.solve(K(X[i]),rhs)

    # Generating Test input and output
    m_test=int(testratio*samples)
    X_test = lhs(n, samples=m_test)
    U_test=np.ones((len(X_test),dof))
    for i in range(len(X_test)):
      U_test[i]=np.linalg.solve(K(X_test[i]),rhs)
    
    inputs =  X
    ouptputs =U

    Building ANN Model in Pytorch

    # Hyper-parameters 
    input_size = 10
    ouput_size = 8
    hidden_size = 20
    num_epochs = 100
    batch_size = 32
    learning_rate = 0.001

    # Fully connected neural network with one hidden layer
    class NeuralNet(nn.Module):
        def __init__(self, input_size, hidden_size, ouput_size):
            super(NeuralNet, self).__init__()
            self.fc1 = nn.Linear(input_size, hidden_size) 
            self.relu = nn.ReLU()
            self.fc2 = nn.Linear(hidden_size, output_size)  

        def forward(self, x):
            out = self.fc1(x)
            out = self.relu(out)
            out = self.fc2(out)
            return out

    model = NeuralNet(input_size, hidden_size, output_size).to(device)

    # Loss and optimizer
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  

    # Train the model
    total_step = len(train_loader)
    for epoch in range(num_epochs):
        for i, (inputs, labels) in enumerate(train_loader):  
            # Move tensors to the configured device
            labels = labels.to(device)

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if (i+1) % 100 == 0:
                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' 
                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))

    # Test the model
    # In test phase, we don't need to compute gradients (for memory efficiency)
    with torch.no_grad():
        correct = 0
        total = 0
        for inputs, labels in test_loader:
            labels = labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        print('Accuracy of the network: {} %'.format(100 * correct / total))

    # Save the model checkpoint
    torch.save(model.state_dict(), 'model.ckpt')
